---
year: 2022
title: "Stochastic Training is Not Necessary for Generalization"
source: "https://openreview.net/forum?id=ZBESeIUB5k"
authors:
  - {name: Jonas Geiping, link: https://jonasgeiping.github.io/}
  - {name: Micah Goldblum, link: }
  - {name: Phil Pope, link: }
  - {name: Michael Moeller, link: https://sites.google.com/site/michaelmoellermath}
  - {name: Tom Goldstein, link: }
publisher: ICLR 2022
image: data\images\GeipingICLR2022.png
links:
  BibTeX: data\bibtex\GeipingICLR2022.bib
  Code: https://github.com/JonasGeiping/fullbatchtraining
---
Models trained with full-batch gradient descent and explicit regularization can match the generalization performance of models trained with stochastic minibatching.
